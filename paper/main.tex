\documentclass[a4paper, 14pt]{article}

\input{preamble.tex}
\renewcommand{\abstractname}{Аннотация}

\title{Способы учёта шума данных в модели нейронных дифференциальных уравнений}

\author{Владимиров Эдуард \\
	\texttt{vladimirov.ea@phystech.edu} \\
	
	\And
	Стрижов Вадим \\
	\texttt{strijov@phystech.edu}
}
\date{\today}

\begin{document}
	\maketitle
	
	\begin{abstract}
		TODO
	\end{abstract}
	
	\keywords{временной ряд \and нейронные обыкновенные дифференциальные уравнения \and нейронные стохастические дифференциальные уравнения \and шум}
	
	\section{Введение}
	Извлечение шума является важной частью предобработки данных, так как он ограничивает процесс извлечения информации. 
	Для того, чтобы нивелировать эффект зашумлённости данных на качество предсказательной модели, необходимо либо заранее предобработать временной ряд, либо внедрить в неё механизм очистки данных, например аугментации, либо, наоборот, добавить шум к чистым данным.
	
	Во многих случаях временные ряды создаются низкоразмерной динамической системой.
	Тогда загрязнение шумом временных рядов может привести к затруднению поиска оптимальной размерности эмбеддинга \cite{kostelich1990noise} и ограничению точности прогнозирования \cite{elshorbagy2002noise}. Поэтому снижение
	уровня шума при сохранении базовой динамики, генерируемой на основе временных рядов, имеет первостепенное значение.
	
	\begin{table}[bhtp]
		\caption{Классификация методов фильтрации временных рядов и их аналогов в фреймворке Neural ODE}
		\begin{tabulary}{\textwidth}{|C|C|C|}
			\hline
			\textbf{Механизм предобработки} & \textbf{Фильтры} & \textbf{Аналог в фреймворке NODE} 
			\\ \hline
			Разложение & вейвлет-преобразование, преобразование Фурье, метод главных компонент & --- 
			\\ \hline
			Итеративный & фильтр Калмана & непрерывный процесс потока времени (Continuous Time Flow Process, CTFP) \cite{Deng2020}, GRU-ODE-Bayes \cite{DeBrouwer2020}
			\\ \hline
			Итеративный & экспоненциальное сглаживание, модель Хольта-Брауна, пакетная нормализация & ---
			\\ \hline
			Введение случайности & dropout, добавление гауссовского шума
			& NSDE с разными компонентами перед дифференциалом от стандартного броуновского движения \cite{Liu2019}
			\\ \hline
		\end{tabulary}
	\end{table}
	
	Существует несколько основных методов предобработки сигналов.
	Фильтр Калмана \cite{Dhwaj2018} использует информацию о физике самого явления.
	Вейвлет преобразование \cite{Alrumaih2002} раскладывает временной ряд на множество других с разным разрешением.
	Экспоненциальное сглаживание \cite{gardner1985exponential} выравнивает временной ряд с помощью экспоненциальной оконной функции.
	В работе \cite{Ignatov2016} фильтрация одномерного временного ряда выполняется в несколько этапов.
	Вначале строится матрица Ханкеля на основе теоремы Такенса \cite{noakes1991takens}.
	Затем выполняется сингулярное разложение этой матрицы с сохранением некоторого числа первых компонент.
	После этого исходный ряд восстанавливается как среднее значение элементов на побочных диагоналях.
	
	Модель нейронных дифференциальных уравнений (Neural Ordinary Differential Equation, NODE) \cite{Chen2018} объединяет в себе нейронные сети и дифференциальные уравнения. 
	Помимо использования в моделировании динамических системах, она также применяется в задачах интерполяции, экстраполяции и генерации временных рядов, в частности в финансах, робототехнике \cite{xie2019neural, meleshkova2021application, du2020model}. 
	Данный механизм является непрерывным аналогом сети ResNet \cite{wu2019wider}, в которой впервые использовались остаточные соединения для решения проблемы затухающих градиентов.
	NODE объединяет в себе плюсы из обоих миров. 
	Структура нейронной сети даёт высокую обобщающую способность, а структура дифференциального уравнения даёт твёрдый теоретический фундамент.
	Однако у этого фреймворка есть и минусы. 
	Существует класс функций, которые не представимы этой моделью \cite{Dupont2019}, а также она плохо устойчива к шуму в данных \cite{Liu2019}.
	
	Для решения вышеизложенных проблем было разработано новое семейство нейронных сетей ~--- стохастические дифференциальные уравнения (Neural Stochastic Differential Equation, NSDE).
	Она внедряет шум в модель NODE с помощью стохастического дифференциального уравнения (SDE).
	Данный фреймворк позволяет эксплуатировать такие регуляризаторы, как Dropout и слой гауссовского шума \cite{liu2018towards, baldi2013understanding}.
	Однако остаётся вопрос: можно ли внедрить другие регуляризаторы (например, BatchNorm) и вышеупомянутые методы фильтрации в фреймворк NODE? 
	В данной работе планируется ответить на этот вопрос.
	
	[TODO: Краткое описание вычислительного эксперимента]
	
	\section{Сопутствующие работы}
	Наша работа вдохновлена успехом модели Neural ODE и её многочисленных модификаций: ANODE, Neural SDE, Neural CDE.
	Мы стремимся перенести некоторые методы фильтрации временных рядов в модель нейронных дифференциальных уравнений.
	Однако такие регуляризаторы, как пакетная нормализация и вейвлет преобразование, тяжело внедрить в Neural ODE ввиду отсутствия скрытой динамики.

	\subsection{Neural ODE}
	Как было сказано ранее, модель нейронных дифференциальных уравнений является непрерывным расширением модели ResNet, в которой параметризуется производная скрытого состояния.
	\begin{equation*} \label{eq:resnet_step}
		\bh_{t+1} = \bh_t + f_{\theta_t}(\bh_t).
	\end{equation*}
	Разность $\bh_{t+1} - \bh_t$ можно интерпретировать как дискретизацию производной $\bh'(t)$ c разницей во времени $\Delta t = 1$. Устремив $\Delta t \rightarrow 0$, получим:
	\begin{equation*} \label{eq:node}
		\lim_{\Delta t \rightarrow 0} \dfrac{\bh_{t + \Delta t} - \bh_t}{\Delta t} = \dfrac{d\bh(t)}{dt} = f_{\theta}(\bh(t), t)
	\end{equation*}
	Модель нейронных дифференциальных уравнений аппроксимирует отображение $\bx \rightarrow \by$ путём обучения нейронной сети $f_\theta$ и линейных отображений $l_\theta^1, \, l_\theta^2$.
	$$ \by \approx l_\theta^2(\bh_T), \text{ где } \bh_T = \bh_0 + \int_{0}^{T} f_\theta(\bh_t) dt \text{ и } \bh_0 = l_\theta^1(\bx)$$
	
	\subsection{Neural SDE}
	Конструкция выглядит следующим образом.
	Зафиксируем момент времени $T > 0$.
	Обозначим за $(\bY_t, t \in [0, T]$ искомый $y$-мерный случайный процесс, где $y$ ~--- размерность данных.
	
	Пусть $(\bW_t, t \in [0, T])$  ~--- $w$-мерный винеровский процесс и $V \sim \mathcal{N}(0, I_v)$ ~--- $v$-мерный стандартный гауссовский вектор. 
	Пусть
	$$ \zeta_\theta: \dR^v \rightarrow \dR^d, \; \mu_\theta: [0, T] \times \dR^d \rightarrow \dR^d, \; \sigma_\theta: [0, T] \times \dR^d \rightarrow \dR^{d \times w}, \; l_\theta: \dR^d \rightarrow \dR^y,$$
	где $d$ ~--- размерность скрытого состояния, $\zeta_\theta, \mu_\theta, \sigma_\theta$ ~--- нейронные сети, $l_\theta$ ~--- линейное преобразование.
	Тогда модель Neural SDE имеет следующий вид:  
	$$ \bh_0 = \zeta_\theta(V), \quad d\bh_t = \mu_\theta(t, \bh_t)dt + \sigma_\theta(t, \bh_t) \circ d\bW_t, \quad \widehat{\bY_t} = l_\theta(\bh_t).$$
	Решением SDE служит случайный $d$-мерный процесс $(\bh_t, t \in [0, T])$.
	Модель обучается таким образом, что $\widehat{\bY} \approx \bY$
	
	В работе \cite{Liu2019} показано, что добавление гауссовского шума выражается следующим образом: $\sigma(t, \bh_t) = \Sigma(t), \:$ где $\Sigma(t)$ ~--- диагональная матрица с элементами, отвечающими за дисперсию шума, добавляемого к вектору скрытого состояния.
	А слой Dropout представляется так: $\sigma(t, \bh_t) = \sqrt{\dfrac{1 - p}{p}} \mu_\theta(t, \bh_t)$, где $p$ ~--- вероятность успеха в схеме испытаний Бернулли. 
	
	\subsection{ResNet}
	Нетрудно заметить, что блок \ref{eq:resnet_step} модели ResNet соответствует одному шагу метода Эйлера численного интегрирования уравнения \ref{eq:node}.
	Учитывая этот факт и то, что существует множество регуляризаторов для данного семейства моделей [ссылки]
	
	\section{Постановка задачи}
	Бегло написать про $\bx_t = \bz_t + \boldsymbol{\epsilon_t}$. 
	Решается задача предсказания временного ряда (вообще зависит от выбранного датасета, возможно будет задача классификации)
	
	\section{Вычислительный эксперимент}
	Основная идея: шум + NODE = КАПУТ, регуляризация = ГУД.
	
	\section{Заключение}
	TODO
	
	\bibliographystyle{unsrtnat}
	\bibliography{references.bib}
	
\end{document}