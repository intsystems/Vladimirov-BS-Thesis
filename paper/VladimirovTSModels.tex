\documentclass[a4paper, 12pt]{article}
\usepackage[]{cite}
\usepackage{cmap}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools,dsfont}
\usepackage{icomma}
\usepackage[dvips]{graphicx}
\usepackage{epsfig}
\usepackage{extsizes}
\usepackage{subfig}
\usepackage{color}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage[square,sort,comma,numbers]{natbib}

\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	urlcolor=cyan,
}

\newcommand\argmin{\mathop{\arg\min}}
\newcommand{\hchi}{\hat{\boldsymbol{\chi}}}
\newcommand{\hphi}{\hat{\boldsymbol{\varphi}}}
\newcommand{\bchi}{\boldsymbol{\chi}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\hx}{\hat{x}}
\newcommand{\hy}{\hat{y}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\p}{p(\cdot)}
\newcommand{\q}{q(\cdot)}
\newcommand{\uu}{\mathbf{u}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bff}{\mathbf{f}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bl}{\mathbf{l}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bsigma}{\boldsymbol{\sigma}}
\newcommand{\bomega}{\boldsymbol{\omega}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bgamma}{\boldsymbol{\gamma}}
\newcommand{\bdelta}{\boldsymbol{\delta}}
\newcommand{\bPsi}{\boldsymbol{\Psi}}
\newcommand{\bpsi}{\boldsymbol{\psi}}
\newcommand{\bxi}{\boldsymbol{\xi}}
\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\bzeta}{\boldsymbol{\zeta}}
\newcommand{\blambda}{\boldsymbol{\lambda}}
\newcommand{\beps}{\boldsymbol{\varepsilon}}
\newcommand{\bZeta}{\boldsymbol{Z}}
% mathcal
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cW}{\mathcal{W}}

\newcommand{\dH}{\mathds{H}}
\newcommand{\dR}{\mathds{R}}
% transpose
\newcommand{\T}{^{\mathsf{T}}}

% command to strike out text
\newcommand{\stkout}[1]{\ifmmode\text{\sout{\ensuremath{#1}}}\else\sout{#1}\fi}

\renewcommand{\epsilon}{\ensuremath{\varepsilon}}
\renewcommand{\phi}{\ensuremath{\varphi}}
\renewcommand{\kappa}{\ensuremath{\varkappa}}
\renewcommand{\le}{\ensuremath{\leqslant}}
\renewcommand{\leq}{\ensuremath{\leqslant}}
\renewcommand{\ge}{\ensuremath{\geqslant}}
\renewcommand{\geq}{\ensuremath{\geqslant}}
\renewcommand{\emptyset}{\varnothing}


\renewcommand{\baselinestretch}{1}


\newtheorem{Th}{Теорема}
\newtheorem{Def}{Определение}
\newenvironment{Proof} % имя окружения
{\par\noindent{\bf Доказательство.}} % команды для \begin
{\hfill$\scriptstyle\blacksquare$} % команды для \end
\newtheorem{Assumption}{Предположение}
\newtheorem{Corollary}{Следствие}
\newtheorem{problem}{Problem}


\textheight=24cm % высота текста
\textwidth=16cm % ширина текста
\oddsidemargin=0pt % отступ от левого края
\topmargin=-1.5cm % отступ от верхнего края
\parindent=24pt % абзацный отступ
\parskip=0pt % интервал между абзацами
\tolerance=2000 % терпимость к "жидким" строкам
\flushbottom % выравнивание высоты страниц

%\graphicspath{ {fig/} }



\begin{document}
	
	\thispagestyle{empty}
	\begin{center}
		\sc
		Министерство образования и науки Российской Федерации\\
		Московский физико-технический институт
		{\rm(государственный университет)}\\
		Физтех-школа прикладной математики и информатики\\
		Кафедра <<Интеллектуальные системы>>\\[35mm]
		\rm\large
		Владимиров Эдуард Анатольевич\\[10mm]
		\bf\Large
		Модели пространства состояний в задачах классификации сигналов ЭКоГ \\[10mm]
		\rm\normalsize
		010990 --- Интеллектуальный анализ данных\\[10mm]
		\sc
		Выпускная квалификационная работа бакалавра\\[10mm]
	\end{center}
	\hfill\parbox{80mm}{
		\begin{flushleft}
			\bf
			Научный руководитель:\\
			\rm
			д.~ф.-м.~н. Стрижов Вадим Викторович\\[5cm]
		\end{flushleft}
	}
	\begin{center}
		Москва\\
		2023
	\end{center}
	
	
	\newpage
	\tableofcontents
	\newpage
	
	\begin{abstract}
		Данная работа посвящена нейронному декодированию ~--- восстановлению стимула по сигналам головного мозга. А именно, рассматривается задача бинарной классификации сигналов, полученных во время движения руки. Для получения высокого качества классификации предлагается использовать модели глубокого обучения. В задаче декодирования сигналов часто применяются свёрточные нейронные сети и трансформеры, в то время как модели пространства состояний применяются редко. Предлагается заполнить данный пробел и подготовить практическое руководство к выбору модели из данного семейства. Проведено сравнение следующих моделей пространства состояний: RNN, NCDE, S4. Для анализа качества алгоритмов предсказания проводится вычислительный эксперимент на данных ЭКоГ.
		
		\bigskip
		\textbf{Ключевые слова}: \emph{ЭКоГ, нейронное декодирование, модели пространства состояний, RNN, S4, NCDE}
	\end{abstract}
	
	\newpage
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section*{Введение}
	\addcontentsline{toc}{section}{\protect\numberline{}Введение}
	
	Нейронное декодирование ~-- процесс расшифровки информации, полученной в результате нейронной активности ~-- имеет большие перспективы в понимании сложностей человеческого мозга и разработке передовых приложений в таких областях, как нейропротезирование и интерфейсы мозг-компьютер.
	Нейронное декодирование включает в себя извлечение значимой информации из данных об активности головного мозга с целью вывода когнитивных состояний, намерений движения или ощущений.
	Например, исследователи предсказывают движения, основываясь на активности в моторной коре \citep{temp_ethier2012restoration}, действия, основанные на активности в префронтальной и теменной коре \citep{temp_ibos2017sequential}, и пространственные местоположения, основанные на активности в гиппокампе \citep{temp_davidson2009hippocampal}. 
	%Эти расшифрованные предсказания могут быть использованы для управления устройствами(например, роботизированной конечностью) или для лучшего понимания того, как области мозга соотносятся с внешним миром.
	Расшифровывание сигналов головного мозга даёт нам понять, как мозг обрабатывает и представляет информацию о мире, открывая путь к революционным достижениям в понимании и взаимодействии с мозгом.

	По сути, нейронное декодирование - это задача классификации (или регрессии), связывающая нейронные сигналы с определенными переменными. При такой постановке проблемы становится очевидным, что существует широкий спектр методов, которые можно
	применить. Однако, несмотря на недавние достижения в области методов машинного
	обучения, по-прежнему принято декодировать активность традиционными методами, такими как логистическая регрессия. Использование современных инструментов ML для нейронного декодирования значительно повышает качество прогноза и глубже проникнуть в функции нервной системы
	Стоит отметить, что модели машинного обучения применяются не только при прогнозировании, но и для предобработки данных от шума.
	Для этого используются свёрточные автоэнкодеры \citep{temp_leite2018deep, temp_caldas2020towards}, фильтр Калмана \citep{paninski2010new} и генеративно-состязательные сети \citep{temp_an2022auto}.
	
	Модели машинного обучения, особенно модели пространства состояний, дают математическую основу для моделирования сложной динамики нейронных систем.
	Модели пространства состояний предлагают гибкий подход, представляя нейронную активность как последовательность скрытых состояний, развивающихся со временем, что обеспечивает структурированное представление, упрощающее извлечение значимой информации.
	
	Используются скрытые марковские цепи \citep{paninski2010new}, линейная модель пространства состояний со скрытыми состояниями \citep{wu2009neural}
	
	Выбор соответствующей модели для конкретной задачи нейронного декодирования является важным, поскольку различные модели обладают отличительными свойствами и характеристиками, которые могут значительно влиять на эффективность декодирования.
	Поэтому крайне важно тщательно изучить и сравнить свойства моделей, чтобы предоставить практическое руководство по её выбору конкретной модели. 
	Понимая сильные и слабые стороны каждой из них, исследователи и практики смогут принимать обоснованные решения при разработке экспериментов и приложений нейронного декодирования.
	
	В данной работе сравниваются следующие модели пространства состояний: рекуррентные нейронные сети (RNN), контролируемые дифференциальные уравнения (NCDE) \citep{ncde}, модель структурного пространства состояний (S4) \citep{s4}
	
	Остальная часть работы организована следующим образом: TODO. %Раздел 2 представляет обзор литературы по нейронному декодированию и моделям пространства состояний. Раздел 3 описывает методологию, используемую в нашем исследовании, включая набор данных и экспериментальную настройку. Раздел 4 представляет подробный анализ и сравнение моделей RNN, S4 и NCDE в задачах нейронного декодирования. Раздел 5 предоставляет практическое руководство по выбору модели на основе их свойств. Наконец, раздел 6 завершает работу, подчеркивая вклад и значение нашей работы и предлагая направления для будущих исследований в области моделей пространства состояний в нейро-декодировании.	
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\newpage
	\section*{Обозначения}
	\addcontentsline{toc}{section}{\protect\numberline{}Обозначения}
	
	\begin{itemize}
		\item ЭКоГ ~--- электрокортикограмма
		\item RNN ~--- Recurrent Neural Network
		\item S4 ~--- Structured State Space for Sequence Modeling
		\item NODE ~--- Neural Ordinary Differential Equation
		\item NCDE ~--- Neural Controlled Differential Equation
		\item МКР ~--- межквартильный размах
	\end{itemize}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\newpage
	\section{Постановка задачи}
	
	Пусть $\bX \in \dR^{M \times N \times T}$ ~--- $M$ измерений ЭКГ, где $N$ ~--- число электродов, $T$ ~--- число элементов временного ряда. Одному измерению ЭКГ соответствуют набор сигналов, записанный на некоторый промежуток времени, во время которого был совершён некий стимул.
	
	$Y \in \{ 0, 1\}^M$ ~--- целевая переменная, индикатор наличия/отсутствия стимула.
	
	Целевая функция $ \bff: \bX \times W \to Y$, где $W \in \dR^P$ ~--- параметры модели
	
	Критерий качества ~--- бинарная кросс-энтропия с $L2$-регуляризацией:  
	
	$$ L(\bw) = -\dfrac{1}{M} \sum\limits_{m=1}^M y_m \log(f(\bw, \bx)) + (1 - y_m) \log(1 - f(\bw, \bx)) + \lambda ||\bw||^2$$
	
	Оптимизационная задача ~--- выбор модели, доставляющей минимум критерия качества: $$\hat{\bw} = \underset{\bw}{\arg \min} \; L(\bw)$$
	
	Основная метрика для этой задачи ~-- точность (accuracy), поскольку классы сбалансированы.
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\newpage
	\section{Обзор существующих алгоритмов классификации}
	Основная задача данной работы ~--- анализ свойств моделей пространства состояний на примере задачи нейронного декодирования сигналов ЭКоГ на два класса ~--- "движение" и "покой". 
	
	На данный момент решение задачи выглядит следующим образом: сигнал фильтруется для удаления шумов, затем применяются преобразования для извлечения признаков (среднее и максимум, параметры Хёрша, вейвлет-преобразование или преобразование Фурье или преобразование Гильберта). После этого применяется алгоритм классификации.
	
	TODO: краткий обзор методов извлечения признаков
	
	\begin{enumerate}
		\item LR, логистическая регрессия \cite{scikit-learn}
		
		Простая линейная модель для решения задачи классификации с L2-регуляризацией.\\
		В данном классификаторе варьируется константа регуляризации.
		
		\item SVM, классификатор на методе опорных векторов \cite{scikit-learn}. 
		
		Модель для классификации с L2-регуляризацией. Алгоритм находит опорные векторы каждого класса, максимизируя расстояние между ними. Гиперплоскость между опорными векторами ~--- граница принятия решения.\\
		В качестве ядра рассматриваются линейная и радиально-базисная функции. Также варьируется константа регуляризации.
		
		\item LDA, линейный дискриминантный анализ.
		
		Линейная классификация в пердположениях, что каждый класс распределён нормально и все классы имеют одинаковую матрицу ковариации.\cite{scikit-learn}
		
		\item ERPCov TS LR
		
		По данным ЭКоГ строятся матрицы ковариаций событийных потенциалов (event-related potentials, ERP) \cite{PyRiemann}. Полученные матрицы проецируются в касательное пространство (TS) \cite{PyRiemann}, после чего проводится классификация с помощью логистической регрессии.\\
		Варьируются тип оценочной функции для ERP ковариаций и константа регуляризации в логистической регрессии.
		
	\end{enumerate}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\newpage
	\section{Модели пространства состояний}
	Модель пространства состояний ~--- это широкое семейство моделей, которое охватывает целый класс частных случаев, представляющих интерес, во многом такой же, как и линейная регрессия, является модель пространства состояний или динамическая линейная модель, которая была представлена в работах Калмана и Бьюси \citep{kalman1961}.
	Модель возникла в настройках космического слежения, где уравнение состояния определяет уравнения движения для положения или состояния космического аппарата с местоположением $x_t$, а данные $u_t$ отражают информацию, которую можно наблюдать с устройства слежения, такую как скорость и азимут.
	Хотя модель была введена как метод, предназначенный главным образом для использования в исследованиях, связанных с аэрокосмической промышленностью, она применялась для моделирования данных из экономики \citep{harrison1976bayesian, harvey1984estimating, harvey1983forecasting} и медицины.
	Отличной трактовкой анализа временных рядов, основанной на модели пространства состояний, является работа \citep{durbin2002simple}. Современную трактовку нелинейных моделей пространства состояний можно найти в работе \citep{douc2014nonlinear}.
	
	Каноничное представление линейной непрерывной модели пространства состояний таково:
	\begin{equation}\label{eq:ssm-cont}
		\begin{aligned}
			\bx'(t) &= A\bx(t) + B\bu(t) \\
			\by(t) &= C\bx(t) + D\bu(t), \\
		\end{aligned}
	\end{equation}

	где 
	$\bx(\cdot) \text{ ~--- вектор состояния}, \bx(t) \in \dR^K$
	
	$\by(\cdot) \text{ ~--- вектор выхода}, \by(t) \in \dR^s$
	
	$\bu(\cdot) \text{ ~--- вектор входа}, \bu(t) \in \dR^d$
	
	$A \text{ ~--- матрица системы}, A \in \dR^{K \times K}$
	
	$B \text{ ~--- матрица входа}, B \in \dR^{K \times d}$
	
	$C \text{ ~--- матрица выхода}, C \in \dR^{s \times K}$
	
	$D \text{ ~--- матрица прямого распространения}, D \in \dR^{s \times d}$. По сути, это слагаемое соответствует SkipConnection в сети ResNet \citep{temp_he2016deep}, и им можно пренебречь. Поэтому далее будем считать $D = 0$
	
	В силу того, что на практике сигналы имеют дискретное представление, то модель \ref{eq:ssm-cont} преобразуется в следующий вид:
	\begin{equation}\label{eq:ssm-discr}
		\begin{aligned}
			\bx_k &= A\bx_{k-1} + B\bu_k \\
			\by_k &= C\bx_k + D\bu_k
		\end{aligned}
	\end{equation}

	Перечислим основные свойства данного семейства моделей:
	\begin{enumerate}
		\item[$+$] обработка временного ряда любой длины
		\item[$+$] количество параметров не зависит от длины последовательности
		\item[$+$] возможность быстро получить предсказание
		\item[$-$] тяжело обучать из-за проблемы взрывающихся/затухающих градиентов и переобучения
		\item[$-$] медленное обучение по сравнению со свёрточными нейронными сетями и трансформерами
	\end{enumerate}

	Далее будут рассмотрены следующие модели пространства состояний: RNN, NCDE и S4.
	Будут подробно описаны их преимущества и недостатки.
	
	Для дальнейшего анализа моделей введём следующие обозначения:
	\begin{itemize}
		\item $d$ ~--- размерность исходного пространства
		\item $K$ ~--- размерность скрытого пространства
		\item $s$ ~--- размерность целевого пространства
		\item $M = \max(d, K, s)$
		\item $L$ ~--- длина обрабатываемой последовательности
	\end{itemize}
	
	\subsection{Рекуррентные нейронные сети (RNN)}
	Рекуррентные нейронные сети являются самой простой и исторически ранней моделью для работы с текстом \citep{rnn}.
	В уравнении \ref{eq:rnn} представлена простая рекуррентная модель ~--- схема Элмана.
	В дальнейшем были разработаны модификации данной схемы, которые устраняют её недостатки ~--- это модели LSTM и GRU \citep{lstm, lstm-and-gru}, и модель двунаправленной рекуррентной нейронной сети \citep{bidirectional-rnn}.
	
	\begin{equation}\label{eq:rnn}
		\begin{aligned}
			\bx_t &= \sigma_x(W_x \bx_{t-1} + W_u \bu_t) \\
			\by_t &= \sigma_y(W_y \bx_t),
		\end{aligned}
	\end{equation}
	$\text{где } \bu_i \in \dR^d, \; \bx_i \in \dR^K \; \by_i \in \dR^s$  
	
	$\sigma_x: \dR^K \rightarrow \dR^K$ ~--- функция активации  
	
	$\sigma_y: \dR^s \rightarrow \dR^s$ ~--- функция активации  
	
	$W_x \in \dR^{K \times K}, W_u \in \dR^{s \times K}, W_y \in \dR^{K \times d}$ ~--- матрицы весов

	\textbf{Cвойства:}
	\begin{itemize}
		\item количество параметров = $K^2 + Ks + Kd = O(KM)$
		\item время прямого прохода = $O(KML)$
	\end{itemize}

	\textbf{Преимущества:}
	\begin{itemize}
		\item есть реализация во всех фреймворках глубокого обучения
		\item относительно быстрое обучение и предсказание модели
	\end{itemize}

	\textbf{Недостатки:}
	\begin{itemize}
		\item не работают с данными, содержащими пропуски и/или разной частотой сэмплирования
	\end{itemize}

	\subsection{Нейронные контролируемые дифференциальные уравнения (NCDE)}
	В модели нейронных обыкновенных дифференциальных уравнений (NODE), прародителе модели NCDE, предполагается, что скрытое состояние описывается дифференциальным уравнением \ref{eq:node}.
	
	\begin{equation}\label{eq:node}
		\bx'(t) = \bff(\bw, \bx(t))
	\end{equation}

	Данный подход позволяет более качественно описывать временной ряд, порождённый динамической системой. Предсказание модели получаются решением дифференциально-
	го уравнения \ref{eq:node} с начальными условиями, в котором правая часть задается нейронной сетью. Это означает, что за один прямой проход алгоритм выдает всю траекторию, в отличие от рекуррентных сетей.
	
	Однако у модели NODE есть и минусы.
	Во-первых, выдаваемое ею решение дифференциального уравнения зависит только от начального состояния, которое постоянно во времени.
	Во-вторых, не существует механизма для дообучения модели на основе новых данных.
	Эти пробелы нивелируются моделью нейронных контролируемых дифференциальных уравнений, представленной уравнением \ref{eq:ncde}
	
	\begin{equation}\label{eq:ncde}
		\begin{aligned}
			\bx(t_1) &= \zeta(\bu_1, t_1) \\
			\bx(t) &= \bx(t_1) + \int_{t_1}^t \bff(\bx(\tau))dU(\tau) \\
			\by_i &= g(\bx(t_i))
		\end{aligned}
	\end{equation}
	$\text{где } \bu_i \in \dR^d, \; \by_i \in \dR^s $
	
	$\bx: [t_1, t_L] \rightarrow \dR^K$ ~--- функция скрытого состояния
	
	$U: [t_1, t_L] \rightarrow \dR^{d+1}$ ~--- кубический сплайн 
	
	$\zeta: \dR^{d+1} \rightarrow \dR^K$ ~--- проектор в скрытое пространство
	
	$f: \dR^K \rightarrow \dR^{K \times (d+1)}$ ~--- динамика скрытого состояния

	$g: \dR^K \rightarrow \dR^s$ ~--- линейное отображение
	
	Теперь, начальное состояние модели NCDE содержит информацию о времени наблюдения первого объекта.
	При решении дифференциального уравнения в интегральной форме используется интеграл Римана-Стильтьесса вместо интеграла Римана, агрегируя таким образом все наблюдения
	
	Уделим особое внимание пути $U(t)$, модифицируя который можно получать разные модели:
	\begin{itemize}
		\item $U(t) = t$ ~--- модель Neural ODE \citep{node}
		\item $U(t) = \sum\limits_{i=1}^{L-1} \bu_{t_i} \cdot I(t_i \leqslant t < t_{i+1})$ ~--- модель ODE-RNN \citep{latent-ode}
		\item $U(t) = \sum\limits_{i=1}^{L-1} \alpha_i(t) \bu_{t_i} + (1-\alpha_i(t)) \bu_{t_{i+1}} \cdot I(t_i \leqslant t < t_{i+1}), \text{где } \alpha_i(t) = \dfrac{t_{i+1} - t}{t_{i+1} - t_i}$ ~--- модель c линейной функцией интерполяции
	\end{itemize}

	Дообучать модель NCDE можно, решая интегральное уравнение с момента $t_{L}$.
	Однако, данная опция доступна не для всех функций пути: для кубического сплайна ~--- нельзя, а для остальных перечисленных ~--- можно.
	Связано это с тем, что при получении новых данных, старый путь может оказаться невалиден, вследствие чего невалидным окажется и состояние $\bx(t_L)$ \citep{ncde-online}. 
	
	Но и у модели Neural CDE есть недостаток: это большое количество параметров, используемых в функции $\bff$.
	Если в качестве $\bff$ использовать линейную функцию, то число параметров равняется $O(K^2d)$, что на порядок больше, чем у RNN.
	В оригинальной статье \citep{ncde} авторы использовали линейную функцию с малым $K$.
	Также можно использовать малоранговое линейное преобразование, использующее $O(K^2 + Kd)$ параметров, но в статье \citep{ncde} показана неэффективность данного подхода.
	
	\textbf{Cвойства:}
	\begin{itemize}
		\item количество параметров = $O(K^2d + Ks)$
		\item время прямого прохода = $O(K^2dL)$
	\end{itemize}
	
	\textbf{Преимущества:}
	\begin{itemize}
		\item работает с данными, в которых содержатся пропуски и которые имеют разную частоту сэмплирования
		\item большая гибкость в настройке модели: выбор архитектуры функции $\bff(\bx)$ и пути $U(t)$
	\end{itemize}
	
	\textbf{Недостатки:}
	\begin{itemize}
		\item в десятки раз медленнее, чем RNN
		\item на текущий момент не существует эффективной реализации
		\item долгая настройка модели
		\item нестабильное обучение при больших $K$, при использовании солверов с адаптивным шагом и функции активации ReLU
	\end{itemize}

	\subsection{Модель структурированного пространства состояний (S4)}
	Данная модель строится из модели пространства состояний в 3 шага. Во-первых, применяется билинейное преобразование для перехода из непрерывной постановки \ref{eq:ssm-cont} в дискретную \ref{eq:ssm-discr} со следующими характеристиками:
	\begin{align*}
		&u_i = u(i\Delta) \\
		&\overline{A} = (I - \dfrac{\Delta}{2} A)^{-1} (I + \dfrac{\Delta}{2} A) \\
		&\overline{B} = (I - \dfrac{\Delta}{2} A)^{-1} \Delta B \\
		&\overline{C} = C \\
		&\overline{D} = 0
	\end{align*}
	где $\Delta$ ~--- шаг дискретизации.
	
	Далее переходим к свёрточному представлению модели \ref{eq:ssm-discr} с вышеуказанными параметрами. Делается это из соображений эффективности по времени.
	\begin{equation*}
		\begin{split}
			\bx_0 &= \overline{B}\bu_0 \\
			\bx_1 &= \overline{A}\overline{B}\bu_0 + \overline{B}\bu_1 \\
			\bx_2 &= \overline{A}^2\overline{B}\bu_0 + \overline{A}\overline{B}\bu_1 + \overline{B}\bu_2 \\
			\cdots 
		\end{split}
		\qquad
		\begin{split}
			\by_0 &= \overline{C}\overline{B}\bu_0 \\
			\by_1 &= \overline{C}\overline{A}\overline{B}\bu_0 + \overline{C}\overline{B}\bu_1 \\
			\by_2 &= \overline{C}\overline{A}^2\overline{B}\bu_0 + \overline{C}\overline{A}\overline{B}\bu_1 + \overline{C}\overline{B}\bu_2 \\
			\cdots 
		\end{split}
	\end{equation*}
	\begin{equation*}
		\begin{split}
			\by_i &= \overline{C}\overline{A}^i\overline{B}\bu_0 + \overline{C}\overline{A}^{i-1}\overline{B}\bu_1 + \ldots + \overline{C}\overline{A}\overline{B}\bu_{i-1} + \overline{C}\overline{B}\bu_i \\
			\by &= \overline{\mathbf{K}} \ast \bu, \text{ где } \overline{\mathbf{K}} = (\overline{C}\overline{B}, \overline{C}\overline{A}\overline{B}, \ldots, \overline{C}\overline{A}^{L-1}\overline{B}) 
		\end{split}
	\end{equation*}

	Таким образом, если ядро свёртки $\overline{\mathbf{K}}$ уже предпосчитано, то можно быстро применить данный слой.
	Однако для матрицы $A$ общего вида вычисление займёт $O(LK^2)$ памяти и $O(LK^3)$ времени (при $d=s=1$).
	Для экономии памяти вместо ядра свёртки хранятся значения производящей функции от данной последовательности в корнях степени $L$ от 1.
	Для уменьшения же временных затрат на матрицу $A$ накладывают ограничения ~--- она должна иметь следующий вид:
	$$ A = \Lambda - PQ^*,$$
	где $\Lambda \in \dR^{K \times K}$ ~--- диагональная матрица, $P, Q \in \dR^{K \times 1}$
	
	И в третьих, с помощью инициализации HIPPO получаются начальные приближения для $\Lambda, P, Q$.
	Инициализация HIPPO имеет следующий вид:
	\begin{equation*}
		A_{nk} = -
		\begin{cases}
			\sqrt{(2n+1)(2k+1)} &\text{ при } n > k \\
			n+1 &\text{ при } n = k \\
			0 &\text{ при } n < k \\
		\end{cases}
	\end{equation*}
	Подробную информацию о каждом шаге можно найти в работе \citep{s4}.
	
	\textbf{Cвойства:}
	\begin{itemize}
		\item количество параметров = $Kd + Ks$
		\item время прямого прохода = $O(KML)$
	\end{itemize}

	\textbf{Преимущества:}
	\begin{itemize}
		\item хорошо работает с данными, содержащими долговременные зависимости
		\item есть эффективная реализация \citep{s4-git}
	\end{itemize}
	
	\textbf{Недостатки:}
	\begin{itemize}
		\item в несколько раз медленнее, чем RNN
		\item не работают с данными, содержащими пропуски и/или разной частотой сэмплирования
	\end{itemize}

	\subsection{Сравнение моделей}
	TODO: написать затравку
	
	\begin{table}[bhtp]
		\begin{tabular}{c|l|l|}
			\cline{2-3}
			\multicolumn{1}{l|}{}             & Parameters & Forward \\ \hline
			\multicolumn{1}{|c|}{RNN}         & $O(KM)$ & $O(KML)$ \\ \hline
			\multicolumn{1}{|c|}{NCDE}  & $O(K^2d + Ks)$ & $O(K^2dL)$ \\ \hline
			\multicolumn{1}{|c|}{S4}          & $O(Kd + Ks)$ & $O(KML)$ \\ \hline
			\multicolumn{1}{|c|}{Transformer} & $O(K^2d + Ks)$ & $O(LK \cdot (L+s))$ \\ \hline
			\multicolumn{1}{|c|}{CNN} & TODO & TODO \\ \hline
		\end{tabular}
	\end{table}

	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\newpage
	\section{Вычислительный эксперимент}
	
	\subsection{Экспериментальные данные}
	Одновременные ЭКоГ-записи были получены от 12 участников (8 мужчин, 4 женщины)
	в ходе непрерывного клинического мониторинга эпилепсии, проводимого в медицинском центре Харборвью в Сиэтле.
	Эти записи длятся 7 $\pm$ 2 дня на каждого участника. Возраст участников составляет 29 $\pm$ 8 лет, и им были имплантированы электроды, преимущественно в одно полушарие (5 правых, 7 левых).
	Задача декодирования заключается в том, чтобы классифицировать события "движения" и "покоя" верхней конечности руки, противоположной полушарию имплантированного электрода.
	События перемещения соответствуют движению запястья, которое произошло по крайней мере через 0.5 с без движения, в то время как события покоя указывают на отсутствие движения ни в одном запястье в течение по крайней мере 3 с.
	Обработка данных ECoG осуществлена с использованием обычных скриптов MNE-Python. 
	Сначала был удалён средний дрейф постоянного тока и высокоамплитудные разрывы. Затем данные ECoG каждого участника подвергались полосовой фильтрации (1-200 Гц), notch-фильтрации и повторной привязке к общей медиане по электродам. 
	Также удалены зашумлённые сигналы на основе аномального стандартного отклонения (> 5 МКР) или эксцесса (> 10 МКР). 
	Затем сгенерированны 10-секундные сегменты ECoG, сосредоточенные вокруг каждого события "движение" и "отдых". 
	Сегменты ECoG с отсутствующими данными или большими артефактами были удалены на основе аномальной спектральной плотности мощности \citep{peterson2021behavioral}.
	Затем частота сигнала была уменьшена до 250 Гц, а временные интервалы сокращены до 2 секунд по центру каждого события.
	Для каждого участника было сбалансировано количество сегментов движения и отдыха в течение каждого дня записи, в результате чего на одного участника пришлось 1155 $\pm$ 568 событий.
	%Положения электродов локализовано с помощью Fieldtrip toolbox в Matlab [59, 60]. 
	
	\subsection{Результаты}
	TOADD
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\newpage
	\section*{Заключение}
	\addcontentsline{toc}{section}{\protect\numberline{}Заключение}
	
	Показано, что рекуррентные нейронные сети, нейронные контролируемые дифференциальные уравнения и модель S4 являются частными случаями модели пространства состояний.
	
	Продемонстрировано, что NCDE имеет лучшее качество на тестовой выборке по сравнению с другими моделями, однако её время обучения сильно превышает время обучения других моделей
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\newpage
	\addcontentsline{toc}{section}{\protect\numberline{}Список литературы}
	\bibliographystyle{unsrtnat}
	\bibliography{references.bib}
	
\end{document} 